{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pXUsL40VDKX",
        "outputId": "483805f3-ca38-4231-8060-dd19a17603c7"
      },
      "id": "1pXUsL40VDKX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e2a5c82",
      "metadata": {
        "scrolled": true,
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "9e2a5c82",
        "outputId": "e4107fe8-8012-4b61-f847-b51174c99c60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/scratch1/e20-fyp-xai-anomaly-detection/CICDataset/Generated-Labelled-Flow/TrafficLabelling /Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-898593066.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should print True if the file exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with your actual file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratch1/e20-fyp-xai-anomaly-detection/CICDataset/Generated-Labelled-Flow/TrafficLabelling /Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the dataset (replace with the correct path if needed)\n",
        "csv_file_path = '/scratch1/e20-fyp-xai-anomaly-detection/CICDataset/Generated-Labelled-Flow/TrafficLabelling /Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
        "print(os.path.exists(csv_file_path))  # Should print True if the file exists\n",
        "\n",
        "dataset = pd.read_csv(csv_file_path, encoding='utf-8')  # Replace with your actual file path\n",
        "\n",
        "\n",
        "# Check for missing values in the entire dataset\n",
        "missing_values = dataset.isnull().sum()\n",
        "\n",
        "# Display columns with missing values and the number of missing values in each column\n",
        "print(\"Missing values per column:\")\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# To get the rows with missing values\n",
        "rows_with_missing_values = dataset[dataset.isnull().any(axis=1)]\n",
        "\n",
        "# Display the number of missing values per row\n",
        "rows_with_missing_count = rows_with_missing_values.isnull().sum(axis=1)\n",
        "\n",
        "# Print the rows with missing values and how many missing in each row\n",
        "print(\"\\nRows with missing values and count of missing values:\")\n",
        "print(rows_with_missing_count)\n",
        "\n",
        "# Optionally, display rows with missing values for further inspection\n",
        "print(\"\\nRows with missing values:\")\n",
        "print(rows_with_missing_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db4264d0",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "db4264d0",
        "outputId": "361c3e9a-ebed-4466-fe13-9e52b9f4ba3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analyzing file: Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
            "Total number of rows: 191033\n",
            "Rows with missing values: 28\n",
            "\n",
            "Analyzing file: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "Total number of rows: 225745\n",
            "Rows with missing values: 4\n",
            "\n",
            "Analyzing file: Wednesday-workingHours.pcap_ISCX.csv\n",
            "Total number of rows: 692703\n",
            "Rows with missing values: 1008\n",
            "\n",
            "Analyzing file: Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3000794/4127186603.py:20: DtypeWarning: Columns (0,1,3,6,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dataset = pd.read_csv(csv_file_path, encoding='ISO-8859-1')  # Use 'ISO-8859-1' or 'utf-8' as needed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of rows: 458968\n",
            "Rows with missing values: 288622\n",
            "\n",
            "Analyzing file: Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
            "Total number of rows: 286467\n",
            "Rows with missing values: 15\n",
            "\n",
            "Analyzing file: Monday-WorkingHours.pcap_ISCX.csv\n",
            "Total number of rows: 529918\n",
            "Rows with missing values: 64\n",
            "\n",
            "Analyzing file: Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
            "Total number of rows: 288602\n",
            "Rows with missing values: 18\n",
            "\n",
            "Analyzing file: Tuesday-WorkingHours.pcap_ISCX.csv\n",
            "Total number of rows: 445909\n",
            "Rows with missing values: 201\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the directory containing CSV files\n",
        "csv_directory = '/scratch1/e20-fyp-xai-anomaly-detection/CICDataset/Generated-Labelled-Flow/TrafficLabelling '\n",
        "\n",
        "# Loop through each file in the directory\n",
        "for filename in os.listdir(csv_directory):\n",
        "    # Check if the file is a CSV\n",
        "    if filename.endswith('.csv'):\n",
        "        # Construct the full file path\n",
        "        csv_file_path = os.path.join(csv_directory, filename)\n",
        "\n",
        "        # Check if the file exists\n",
        "        if os.path.exists(csv_file_path):\n",
        "            print(f\"\\nAnalyzing file: {filename}\")\n",
        "\n",
        "            try:\n",
        "                # Load the dataset with a fallback encoding\n",
        "                dataset = pd.read_csv(csv_file_path, encoding='ISO-8859-1')  # Use 'ISO-8859-1' or 'utf-8' as needed\n",
        "\n",
        "                # Check for missing values in the entire dataset\n",
        "                missing_values = dataset.isnull().sum()\n",
        "\n",
        "                # Count total rows and rows with missing values\n",
        "                total_rows = len(dataset)\n",
        "                rows_with_missing_values = dataset[dataset.isnull().any(axis=1)]\n",
        "                rows_with_missing_count = len(rows_with_missing_values)\n",
        "\n",
        "                # Display file name, total rows, and number of rows with missing values\n",
        "                print(f\"Total number of rows: {total_rows}\")\n",
        "                print(f\"Rows with missing values: {rows_with_missing_count}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file {filename}: {e}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"File not found: {csv_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e7319f0-b875-4324-b121-42762814cfb4",
      "metadata": {
        "id": "9e7319f0-b875-4324-b121-42762814cfb4",
        "outputId": "e7445d8e-72e1-455d-f634-16439f1ec06e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File '/scratch1/e20-fyp-xai-anomaly-detection/CICDataset/Generated-Labelled-Flow/TrafficLabelling /Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv' loaded successfully.\n",
            "\n",
            "\n",
            "Missing values per column:\n",
            "                 Missing Values  Missing Percentage\n",
            "Flow Bytes/s             288622           62.884994\n",
            " Source IP               288602           62.880637\n",
            " Source Port             288602           62.880637\n",
            " Destination IP          288602           62.880637\n",
            "Flow ID                  288602           62.880637\n",
            "...                         ...                 ...\n",
            "Idle Mean                288602           62.880637\n",
            " Idle Std                288602           62.880637\n",
            " Idle Max                288602           62.880637\n",
            " Idle Min                288602           62.880637\n",
            " Label                   288602           62.880637\n",
            "\n",
            "[85 rows x 2 columns]\n",
            "\n",
            "Number of rows with missing values: 288622\n",
            "\n",
            "No issues with reading specific columns.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the Thursday dataset\n",
        "thursday_file_path = '/scratch1/e20-fyp-xai-anomaly-detection/CICDataset/Generated-Labelled-Flow/TrafficLabelling /Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv'\n",
        "\n",
        "# Try to load the dataset with proper error handling\n",
        "try:\n",
        "    # Attempt to read the dataset with encoding that handles issues\n",
        "    dataset = pd.read_csv(thursday_file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "    print(f\"File '{thursday_file_path}' loaded successfully.\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading file {thursday_file_path}: {e}\")\n",
        "    dataset = None\n",
        "\n",
        "# If dataset loaded successfully, proceed to analyze missing values\n",
        "if dataset is not None:\n",
        "    # Check for missing values per column\n",
        "    missing_values = dataset.isnull().sum()\n",
        "\n",
        "    # Get the total number of rows\n",
        "    total_rows = len(dataset)\n",
        "\n",
        "    # Display missing values and the percentage of missing values\n",
        "    print(\"\\nMissing values per column:\")\n",
        "    missing_percentage = (missing_values / total_rows) * 100\n",
        "    missing_info = pd.DataFrame({'Missing Values': missing_values, 'Missing Percentage': missing_percentage})\n",
        "\n",
        "    # Print columns with missing values\n",
        "    print(missing_info[missing_info['Missing Values'] > 0].sort_values(by='Missing Values', ascending=False))\n",
        "\n",
        "    # Optional: Check rows with the most missing values\n",
        "    rows_with_missing_values = dataset[dataset.isnull().any(axis=1)]\n",
        "    print(f\"\\nNumber of rows with missing values: {len(rows_with_missing_values)}\")\n",
        "\n",
        "    # Check specific columns that could cause reading problems due to encoding or type issues\n",
        "    problematic_columns = []\n",
        "    for column in dataset.columns:\n",
        "        try:\n",
        "            dataset[column].apply(str)  # Try to convert the column to string\n",
        "        except Exception as e:\n",
        "            problematic_columns.append((column, str(e)))\n",
        "\n",
        "    # Print columns with errors\n",
        "    if problematic_columns:\n",
        "        print(\"\\nColumns with errors during reading:\")\n",
        "        for col, error in problematic_columns:\n",
        "            print(f\"Column: {col}, Error: {error}\")\n",
        "    else:\n",
        "        print(\"\\nNo issues with reading specific columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc299998-057a-4517-b371-cbe031f42afc",
      "metadata": {
        "id": "fc299998-057a-4517-b371-cbe031f42afc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (venv)",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}